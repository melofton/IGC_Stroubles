---
title: "Storm Identifier"
author: "Mary Lofton and Colin Baciocco"
date: "7/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages(pacman)
pacman::p_load( tidyverse, lubridate)
```


```{r Read in Bridge 1 data and add a flagging column for high discharge "storm" values}

B1 <- read_csv("../Data_QAQC/Colin/B1.csv") %>%
  mutate( DateTime = parse_date_time( DateTime, "ymd H M S"),
        #Create a new column where 1 demonstrates discharge being above the 90th percentile, 0 means it is not
          Flag_High_Discharge = ifelse( Discharge > quantile( Discharge, 0.9, names = FALSE), 1, 0) )
```



```{r Identify storms by adding a "Storm_ID" column holding each storm's start date. STORM dataset version}
#create a dataset which only holds rows corresponding to "storms" (i.e. rows where discharge is at or above the 90th quantile )
stormdata <- B1 %>% filter(Flag_High_Discharge == 1)

#create a column to hold the storm identifier values.
stormdata$Storm_ID <- paste( stormdata$DateTime[1] )

#create a counter for storm ID
storm_ID <- paste( stormdata$DateTime[1] ) 

#loop through storm dataset and assign identifiers. As the value assigned to each row depends on values in the row before it, the loop starts on the second row. We initially filled the Storm_ID column with storm DateTimes to avoid the first row's storm identifier being blank.
for(i in 2:nrow(stormdata) ){
     if( as.numeric(stormdata$DateTime[i]) - as.numeric(stormdata$DateTime[i-1]) >= 3600 ){
          storm_ID <- paste( stormdata$DateTime[i] )
          stormdata[i,"Storm_ID"] <- storm_ID
        } else {
          stormdata[i,"Storm_ID"] <- storm_ID
        }
}

rm( i, storm_ID)
```


```{r Identify storms by adding a "Storm_ID" column holding each storm's start date. FULL dataset version}
#This version identifies a storm both when discharge drops below the 90th percentile, and when it's been over an hour since the last "storm" row. To do this, it works with the full, finalized dataset instead of only the portion including storms. 
#We identify the same number of storms with this method (I don't think there are all that many events where discharge bottoms out RIGHT below the 90th percentile and then jumps back up over it within the hour), but it's good to be thorough.


# #create a dataset which only holds rows corresponding to "storms" (i.e. rows where discharge is at or above the 90th quantile )
# stormdata <- B1 %>% filter(Flag_High_Discharge == 1)

#create a column to hold the storm identifier values.
B1$Storm_ID <- NA

#create a counter for storm ID
storm_ID <- NA

#create a counter for the date of the last "storm" row
last_stormr <- 0

#loop through storm dataset and assign identifiers. As the value assigned to each row depends on values in the row before it, the loop starts on the second row. This skipping may mean that the dataset starts with a storm the first row may be unidentified, but we manually correct the error with a line of code below.
for(i in 2:nrow(B1) ){
  
  if( B1$Flag_High_Discharge[i] == 1 ){
     
    if( as.numeric(B1$DateTime[i]) - as.numeric(last_stormr) >= 3600 ){
          storm_ID <- paste( B1$DateTime[i] )
          B1[i,"Storm_ID"] <- storm_ID
        } else {
          B1[i,"Storm_ID"] <- storm_ID
        }
    last_stormr <- B1$DateTime[i]
  }
}

#Correct the ID for the first row, if it's a storm 
if( B1$Flag_High_Discharge[1] == 1 )
{
  B1[ 1, "Storm_ID" ] <- paste( B1$DateTime[1] )
}


rm( i, storm_ID, last_stormr)
```