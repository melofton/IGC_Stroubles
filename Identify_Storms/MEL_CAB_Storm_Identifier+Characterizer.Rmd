---
title: "Storm Identifier"
author: "Mary Lofton and Colin Baciocco"
date: "7/10/2019"
output: html_document
---

#This code identifies "storms" (continuous periods of high discharge). To record them, it adds a cell in the column "Storm_ID" containing the start date and time of the storm to each row within the storm event.
#Na values within "Storm_ID" mean that the row is not within a storm. Further code using Storm_ID should reflect this, of course.
#This also contains a section of code calculating each storm's discharge volume. It adds the measurement to a new column, "Vol_m^3," for each row within the storm's time period.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages(pacman)
pacman::p_load( tidyverse, lubridate)
```


```{r Read in QAQC-ed Bridge 1 data and add a flagging column for high discharge "storm" values}

B1 <- read_csv("../Data_QAQC/Colin/B1.csv") %>%
  mutate( DateTime = parse_date_time( DateTime, "ymd H M S"),
        #Create a new column where 1 demonstrates discharge being above the 90th percentile, 0 means it is not
          Flag_High_Discharge = ifelse( Discharge > quantile( Discharge, 0.99, names = FALSE), 1, 0) )
```



```{r Identify storms by adding a "Storm_ID" column holding each storm's start date. STORM dataset version}
#create a dataset which only holds rows corresponding to "storms" (i.e. rows where discharge is at or above the 90th quantile )
stormdata <- B1 %>% filter(Flag_High_Discharge == 1)

#create a column to hold the storm identifier values.
stormdata$Storm_ID <- paste( stormdata$DateTime[1] )

#create a counter for storm ID
storm_ID <- paste( stormdata$DateTime[1] ) 

#loop through storm dataset and assign identifiers. As the value assigned to each row depends on values in the row before it, the loop starts on the second row. We initially filled the Storm_ID column with storm DateTimes to avoid the first row's storm identifier being blank.
for(i in 2:nrow(stormdata) ){
     if( as.numeric(stormdata$DateTime[i]) - as.numeric(stormdata$DateTime[i-1]) >= 3600 ){
          storm_ID <- paste( stormdata$DateTime[i] )
          stormdata[i,"Storm_ID"] <- storm_ID
        } else {
          stormdata[i,"Storm_ID"] <- storm_ID
        }
}

rm( i, storm_ID)
```


```{r Identify storms by adding a "Storm_ID" column holding each storm's start date. FULL dataset version}
#This version identifies a storm both when discharge drops below the 90th percentile, and when it's been over an hour since the last "storm" row. To do this, it works with the full, finalized dataset instead of only the portion including storms. 
#We identify the same number of storms with this method (I don't think there are all that many events where discharge bottoms out RIGHT below the 90th percentile and then jumps back up over it within the hour), but it's good to be thorough.


# #create a dataset which only holds rows corresponding to "storms" (i.e. rows where discharge is at or above the 90th quantile )
# stormdata <- B1 %>% filter(Flag_High_Discharge == 1)

#create a column to hold the storm identifier values.
B1$Storm_ID <- NA

#create a counter for storm ID
storm_ID <- NA

#create a counter for the date of the last "storm" row
last_stormr <- 0

#loop through storm dataset and assign identifiers. As the value assigned to each row depends on values in the row before it, the loop starts on the second row. This skipping may mean that the dataset starts with a storm the first row may be unidentified, but we manually correct the error with a line of code below.
for(i in 2:nrow(B1) ){
  
  if( B1$Flag_High_Discharge[i] == 1 ){
     
    if( as.numeric(B1$DateTime[i]) - as.numeric(last_stormr) >= 3600 ){
          storm_ID <- paste( B1$DateTime[i] )
          B1[i,"Storm_ID"] <- storm_ID
        } else {
          B1[i,"Storm_ID"] <- storm_ID
        }
    last_stormr <- B1$DateTime[i]
  }
}

#Correct the ID for the first row, if it's a storm 
if( B1$Flag_High_Discharge[1] == 1 )
{
  B1[ 1, "Storm_ID" ] <- paste( B1$DateTime[1] )
}


#format Storm_ID to not contain a colon and thus be easily printable
B1 <- mutate( B1, Storm_ID = gsub(":",".", Storm_ID) )


rm( i, storm_ID, last_stormr)
```


```{r calculate volume of each storm. Add volume measurement to its own column within each storm.}

#create a new column in B1 to record storm volume
B1$Storm_Vol <- NA

#get a unique list of storm starts. Ensure Na is not included.
stormsstarts <- B1$Storm_ID[!is.na(B1$Storm_ID)] %>% unique()

for( ID in stormsstarts ){
  stormdata <- filter( B1, Storm_ID == ID )

  vol <- integrate( splinefun( stormdata$DateTime, stormdata$Discharge ),
                    stormdata$DateTime[1], stormdata$DateTime[nrow(stormdata)], 
                    #It may be possible to reduce the number of subdivisions to get this to speed up.
                    subdivisions = nrow(stormdata) + 10 )
  
  B1[ B1$Storm_ID %in% ID, "Storm_Vol" ] <- vol$value
}


rm( stormsstarts, stormdata, ID, vol)
```


```{r Use B1 to create an (easily viewable) tibble with information on each storm, like volume, time length, magnitude (volume / timelength)}

vol_extracter <- function( a ){
  a[1]
}

#This returns the timelength in seconds
timelength_extracter <- function( a ){
  a[length(a)] %>% as.numeric() - a[1] %>% as.numeric()
}

storms_info <- B1 %>% group_by( Storm_ID ) %>%
  summarize( Vol_mcubed = vol_extracter(Storm_Vol), length_H = timelength_extracter(DateTime) / 3600, Intensity = Vol_mcubed / length_H ) %>%
  #drop the "NA" row which summarizes all non-storm events
  filter( !is.na(Storm_ID) )

rm( vol_extracter, timelength_extracter)

storms_info
```

```{r save the storm summary table}
write_csv(storms_info,"./Storm_Summaries.csv")
```