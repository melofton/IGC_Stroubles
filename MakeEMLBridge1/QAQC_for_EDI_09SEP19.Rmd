---
title: "QAQC_LLW_CAB"
author: "Wind & Baciocco"
date: "5/1/2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages(pacman)
pacman::p_load(tidyverse, lubridate)
```


#read in Bridge WQ 1 data
```{r}
B1_WQ <- read_csv("../6_14_2019 Bridge_1 WQ.csv", skip = 1) %>%
  #get rid of rows with unit descriptions
  slice(-c(1:2)) %>%
  #make sure numbers are recognized as numbers, dates as dates, etc.
  #The "meridiem offset" in the older code doesn't seem to be needed. It looks like times are pulled correctly from the excel files.
  mutate(DateTime = as.POSIXct(TIMESTAMP,format="%m/%d/%Y %H:%M",tz=Sys.timezone()),
         Record_number = as.double(RECORD),
         Temp_degC = as.double(Temp),
         SpCond_uScm = as.double(Con)*1000,
         pH = as.double(pH),
         Turb_NTU = as.double(Turb),
         DO_pct = as.double(DOpct),
         DO_mgL = as.double(DOmgl),
         Batt_V = as.double(Batt)) %>%
  # Adjust DateTime with AM/PM
  # mutate(DateTime = Date + meridiemoffset*3600) %>%
 #select only the columns we want
  select(DateTime, Record_number, Temp_degC, SpCond_uScm, pH, Turb_NTU, DO_pct, DO_mgL, Batt_V) %>%
  mutate(Bridge = 1) %>%
  #It seemed like all the WQ for repeated DateTimes were the same... I thought it would be okay to drop every row with a repeated date time after the first row. (See "duplicates.R" for verification that every row with a repeated date time also has repeated WQ sensor values.)
  distinct( DateTime, .keep_all = TRUE)
```

```{r}
write.csv(B1_WQ, file = "B1_WQ_9_2_2019.csv")
```


#Read-in, interp, and QAQC for Bridge 1 stage
```{r echo=FALSE}

B1_stage <- read_csv("../6_14_2019 Bridge_1 Stage.csv", skip = 1) %>%
  #get rid of rows with unit descriptions
  slice(-c(1:2)) %>%
  #make sure numbers are recognized as numbers, dates as dates, etc.
  mutate(DateTime = as.POSIXct(TIMESTAMP,format="%m/%d/%Y %H:%M",tz=Sys.timezone() ),
         Record_number = as.double(RECORD),
         Lvl_m = as.double(Lvl_m)) %>%
  #calculate datetime from date, time, AM/PM
  #mutate(DateTime = Date + Time + meridiemoffset*3600) %>%
  #select only the columns we want
  select(DateTime, Record_number, Lvl_m) %>%
  #filter out every row with a repeated DateTime after the first. Again, see "duplicates.R" for a more in-depth examination of why duplicate rows in this data.
  distinct( DateTime, .keep_all = TRUE ) %>%  
  #Also filter out NaN values for stage.
  filter( !is.na(Lvl_m) )

write.csv(B1_stage, file = "B1_Stage_9_2_2019.csv")



#Make Bridge 1 stage observations (taken every 10 min) compatible with WQ data taken every 15
#"loess()", which actually comes up with a polynomial to fit the local area, could be a possibly better, possibly slightly more accurate way to generate interpolated values.
#This section works in two parts:
#First, it generates a sheet of stage values linearly interpolated between the 10 and 20 or 40 and 50 minute marks.
#Second, it merges those values (placed "on the 45 & 15") with the actual stage values which were recorded on the hour and 30 minute marks.
#Because the stage data set starts earlier than WQ by part of a year, we only see interpolated values a bit of the way down in the merged dataset.
B1_intrp_stage <- approx( x = B1_stage$DateTime, y = B1_stage$Lvl_m,
                          #This filter function in xout ensures that approx() doesn't generalize too much. It filters B1_WQ's dates for those which fall in hours where there are actual stage values to generate a linear rule. The "unit" parameter below can be changed to make this more or less exact.
                          xout = filter( B1_WQ["DateTime"], any( ceiling_date(DateTime, unit = "hours") %in% ceiling_date(B1_stage$DateTime, unit = "hours"))) %>% pull(),
                          method = "linear", rule = 2 ) %>% 
  tbl_df() %>%
  #drop entries that are on the hour or the 30.
  filter( minute(x) == 15 | minute(x) == 45 )
  #rename columns, no pipe operator needed
  colnames(B1_intrp_stage) <- c("DateTime", "Lvl_m")

#this creates stage on the 15 and 30 min intervals  
write.csv(B1_intrp_stage, file = "B1_intrp_stage.csv")

#Merge the interpolated values with the actual stage values measured on the hours and the 30s, arrange the sheet, and record it.
B1_stage_2 <- add_row( filter( B1_stage, minute(DateTime) != 10 & minute(DateTime) != 20 & minute(DateTime) != 40 & minute(DateTime) != 50 ),
                         DateTime = B1_intrp_stage$DateTime, Lvl_m = B1_intrp_stage$Lvl_m ) %>%
  arrange(DateTime)

write.csv(B1_stage_2, file = "B1_Stage_9_2_2019.csv")

B1_stage_QA <- B1_stage_2 %>%
  mutate( bridge = 1, Flag_Lvl_m = ifelse(Lvl_m < 0.1 | Lvl_m > 4, 1, 0))

write.csv(B1_stage_QA, file = "B1_stage_QA.csv")
   

```



#Bridge 1 WQ, stage, and discharge merged into one sheet. Flag and finalize the merged sheet
```{r}

stageToDischarge <- function( a ) {
    
    if( a < 0.7 ){
        return( 4.89292 * a ^ 1.9887 )
    } else {
        return( 13.599 * a - 7.0945 )
    }
}


#calculate and add discharge to stage sheet
B1_stage_QA$Discharge <- sapply( B1_stage_QA$Lvl_m, stageToDischarge)

rm(stageToDischarge)

#cut WQ data down to just those rows for which we have stage and discharge data
#(It shouldn't drop too much -- just 15,000 or so rows out of ~230,000.)
B1_WQ <- B1_WQ[ B1_WQ$DateTime %in% B1_stage_QA$DateTime, ]


#Add discharge and stage, along with stage flags, to the main sheet. Entries are matched by DateTime
B1_WQ <- B1_WQ %>% mutate( Lvl_m = B1_stage_QA[ B1_stage_QA$DateTime %in% B1_WQ$DateTime, ][["Lvl_m"]],
                           Discharge = B1_stage_QA[ B1_stage_QA$DateTime %in% B1_WQ$DateTime, ][["Discharge"]],
                           Flag_Lvl_m = B1_stage_QA[ B1_stage_QA$DateTime %in% B1_WQ$DateTime, ][["Flag_Lvl_m"]])

#Old way to do method in "mutate" is below. They do the same thing -- just experimenting here
# B1_WQ$Lvl_m <- B1_stage_QA$Lvl_m[B1_stage_QA$DateTime %in% B1_WQ$DateTime]
# B1_WQ$Discharge <- B1_stage_QA$Discharge[B1_stage_QA$DateTime %in% B1_WQ$DateTime]
# B1_WQ$Flag_Lvl_m <- B1_stage_QA$Flag_Lvl_m[B1_stage_QA$DateTime %in% B1_WQ$DateTime]



#switching the location of discharge and the the first flag column might be nice.
#Do that here?



#Flag values if they're too high or low. 1 == flagged, 0 == not-flagged
B1_QA <- B1_WQ %>%
  mutate(Flag_Temp_degC = ifelse(Temp_degC < -5 | Temp_degC > 30, 1, 0),
         Flag_SpCond_uScm = ifelse(SpCond_uScm < 50 | SpCond_uScm > 2000 | Lvl_m < 0.25 , 1, 0),
         Flag_pH = ifelse(pH <6 | pH >9 | Lvl_m <0.2, 1, 0),
         Flag_Turb_NTU = ifelse(Turb_NTU < 0 | Turb_NTU > 2000 | Lvl_m < 0.16 , 1, 0), #Timpano et al. 2010
         Flag_DO_pct = ifelse(DO_pct < 0 | DO_pct > 150, 1, 0),
         #Perhaps play around with flagging DO if 0 or something else?
         Flag_DO_mgL = ifelse(DO_mgL < 4 | DO_mgL > 18, 1, 0),
         Flag_Batt_V = ifelse(Batt_V < 11 | Batt_V > 13, 1, 0)) %>%
#Flag values  if it seems like the Stroubles wasn't high enough to cover the sensor on the sonde.
  mutate(Flag_SpCond_uScm = ifelse( Lvl_m < 0.25 , 2, Flag_SpCond_uScm),
         Flag_pH = ifelse( Lvl_m <0.2, 2, Flag_pH),
         Flag_Turb_NTU = ifelse( Lvl_m < 0.16 , 2, Flag_Turb_NTU))
         #For turb, see Timpano et al. 2010
  

#Drop rows that with values that are flagged by the value themselves
B1_QA <- B1_QA %>%
  filter(Flag_Temp_degC != 1,
         Flag_SpCond_uScm != 1,
         Flag_pH != 1,
         Flag_Turb_NTU != 1,
         Flag_DO_pct != 1,
         Flag_DO_mgL != 1,
         Flag_Batt_V != 1) %>%
#Drop values that are flagged by height, but don't drop their row
  mutate(SpCond_uScm = ifelse( Flag_SpCond_uScm == 2, NaN, SpCond_uScm),
         pH = ifelse( Flag_pH == 2, NaN, pH),
         Turb_NTU = ifelse( Flag_pH == 2, NaN, Turb_NTU))

write.csv(B1_QA, file = "B1_QA_9_2_2019.csv", row.names = FALSE)
#215,572 obs to 146,034 = 67.7% remaining
#this does not remove the rows that are <0.1 m for stage... which needs to be done. 
```

#Rename "B1_QA" "B1." This the name every other "colin file" assumes the water quality dataset is called. 
```{r}
B1 <- B1_QA

```

#Save the merged, quality-assured file in the "Colin" QAQC Directory
```{r}
#The QAQC-ed sheet may be shorter than original stage and water quality sheets, but again -- that's because we drop rows with even a single "suspect" value (flagged based on sensor value), per Colin's instructions from the IGC group.
write_csv(B1_QA, "B1.csv")
```